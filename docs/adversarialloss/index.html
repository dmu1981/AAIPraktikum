

<!DOCTYPE html>
<html class="writer-html5" lang="de" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Adversarial Loss &mdash; Advances in AI Praktikum  Dokumentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />

  
    <link rel="canonical" href="https://dmu1981.github.io/MPTPraktikum/adversarialloss/index.html" />
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=245627df"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script src="../_static/translations.js?v=79cc9f76"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="../genindex.html" />
    <link rel="search" title="Suche" href="../search.html" />
    <link rel="prev" title="Perceptual Loss" href="../perceptualloss/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Advances in AI Praktikum
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Dokumentation durchsuchen" aria-label="Dokumentation durchsuchen" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Aufgaben:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pytorch/index.html">PyTorch - Grundlagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/cnn.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../checkpoints/index.html">Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorboard/index.html">Tensor Board</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resnet/index.html">ResNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../embeddings/index.html">Embeddings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../perceptualloss/index.html">Perceptual Loss</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Adversarial Loss</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#das-neue-setup">Das neue Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="#das-zero-sum-spiel">Das Zero-Sum Spiel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alternierendes-training-von-generator-und-kritiker">Alternierendes Training von Generator und Kritiker</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainingsschleife-fur-adversariales-upscaling">Trainingsschleife für adversariales Upscaling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pseudocode-algorithmisch">Pseudocode (algorithmisch):</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#der-generator">Der Generator</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#adversarialloss.misc.ResNetBlock.__init__"><code class="docutils literal notranslate"><span class="pre">ResNetBlock.__init__()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#aufgabe-1-generator-architektur-implementieren"><strong>Aufgabe 1</strong>: Generator-Architektur implementieren</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#adversarialloss.main.Generator"><code class="docutils literal notranslate"><span class="pre">Generator</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#adversarialloss.main.Generator.__init__"><code class="docutils literal notranslate"><span class="pre">Generator.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#adversarialloss.main.Generator.forward"><code class="docutils literal notranslate"><span class="pre">Generator.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Advances in AI Praktikum</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Adversarial Loss</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/adversarialloss/index.rst.txt" rel="nofollow"> Quelltext anzeigen</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="adversarial-loss">
<h1>Adversarial Loss<a class="headerlink" href="#adversarial-loss" title="Link to this heading"></a></h1>
<p>In vorherigen Abschnitten haben wir gesehen, wie sich die Qualität hochskalierter Bilder durch klassische Verlustfunktionen wie den Total Variation Loss und den Perceptual Loss (z. B. mit VGG16-Features) verbessern lässt. Diese Methoden optimieren bestimmte Bildmerkmale – z. B. Glattheit oder semantische Ähnlichkeit – und führen zu visuell ansprechenderen Resultaten als einfache Pixel-basierte Fehlermaße wie MSE oder MAE.</p>
<p>Allerdings stößt diese Optimierung an ihre Grenzen: Auch wenn das rekonstruierte Bild laut Perceptual-Loss „ähnlich“ zum Original ist, fehlt oft der visuelle Realismus, den unser menschliches Auge bei natürlichen Bildern erwartet. Diese Diskrepanz entsteht, weil die verwendeten Verlustfunktionen nicht explizit modellieren, wie ein <em>realistisches Bild</em> aussehen sollte – sie bestrafen lediglich Abweichungen vom Original in bestimmten Merkmalen.</p>
<p>Hier kommt der <em>Adversarial Loss</em> ins Spiel. Anstatt das hochskalierte Bild nur auf Basis vordefinierter Fehlermaße zu optimieren, wird das Problem als ein Spiel zwischen zwei Netzwerken formuliert: einem Generator, der Bilder erzeugt, und einem Diskriminator, der versucht zu erkennen, ob ein Bild real oder künstlich ist. Diese GAN-ähnliche Architektur führt dazu, dass der Generator lernt, Bilder zu erzeugen, die nicht nur „ähnlich genug“ sind, sondern statistisch realistisch wirken – so wie echte Bilder aus der Trainingsverteilung.</p>
<p>Kurz gesagt: Während TV- und Perceptual Loss eher lokale oder semantische Fehler reduzieren, ermöglicht der Adversarial Loss eine globale Verbesserung des Bildrealismus. Er verschiebt den Fokus von „rekonstruiere das Original“ hin zu „täusche den Betrachter“. Für das Upscaling bedeutet das oft: schärfere Kanten, realistischere Texturen und weniger künstlich wirkende Artefakte – insbesondere in fein strukturierten Bildbereichen wie Haaren, Gras oder Texturen.</p>
<p>Im nächsten Abschnitt implementieren wir diesen Ansatz mithilfe eines einfachen GAN-Setups.</p>
<section id="das-neue-setup">
<h2>Das neue Setup<a class="headerlink" href="#das-neue-setup" title="Link to this heading"></a></h2>
<p>Im bisherigen Setup (vgl. frühere Aufgabe zum Perceptual Loss) haben wir ein einzelnes Netzwerk trainiert, um hochskalierte Bilder zu erzeugen. Dieses Netzwerk wurde mit dem Perceptual Loss optimiert, um die Qualität der Ergebnisse zu verbessern.</p>
<a class="reference internal image-reference" href="../_images/srcnn_prevloss.png"><img alt="../_images/srcnn_prevloss.png" class="align-center" src="../_images/srcnn_prevloss.png" style="width: 600px;" />
</a>
<p>Der Adversarial Loss hingegen erfordert zwei Netzwerke: einen Generator und einen Kritiker. Die Aufgabe des Kritikers (Diskriminator) ist es,
zwischen echten Bildern und den vom Generator erzeugten Bildern zu unterscheiden. Er tut dies jedoch nicht in Form eine binären Klassifikation, sondern bewertet die Realitätsnähe der Bilder auf einer Skala.
Dies ermöglicht eine differenziertere Rückmeldung an den Generator. Der Kritiker vergibt in gewisserweise eine Punktzahl für die Qualität der Bilder, anstatt nur zu sagen, ob sie echt oder gefälscht sind.
Dabei sollen möglichst realistische Bilder eine hohe Punktzahl erhalten und weniger realistische Bilder eine niedrige Punktzahl.
Der Kritiker wird also darauf trainiert, echte Bilder von gefälschten zu unterscheiden und dabei eine Art „Qualitätsbewertung“ abzugeben.</p>
<p>Der Generator hingegen versucht, den Kritiker zu täuschen, indem er Bilder erzeugt, die so realistisch wie möglich wirken. Er verwendet
die bisherigen Techniken wie den Perceptual Loss, um die Qualität der Bilder zu verbessern, aber zusätzlich wird er durch den Adversarial Loss motiviert, Bilder zu erzeugen, die der
Kritiker als realistisch bewertet.</p>
<a class="reference internal image-reference" href="../_images/srcnn_bothloss.png"><img alt="../_images/srcnn_bothloss.png" class="align-center" src="../_images/srcnn_bothloss.png" style="width: 600px;" />
</a>
</section>
<section id="das-zero-sum-spiel">
<h2>Das Zero-Sum Spiel<a class="headerlink" href="#das-zero-sum-spiel" title="Link to this heading"></a></h2>
<p>Das Training mit Adversarial Loss kann als ein Zero-Sum Spiel zwischen dem Generator und dem Kritiker betrachtet werden.
Während der Generator versucht, die Punktzahl des Kritikers zu maximieren, indem er realistische Bilder erzeugt, versucht der
Kritiker gleichzeitig, die Punktzahl des Generators zu minimieren, indem er gefälschte Bilder korrekt identifiziert.
Dieses Spiel führt zu einem ständigen Wettlauf zwischen den beiden Netzwerken, wobei beide versuchen, sich gegenseitig zu
überlisten und zu verbessern.</p>
</section>
<section id="alternierendes-training-von-generator-und-kritiker">
<h2>Alternierendes Training von Generator und Kritiker<a class="headerlink" href="#alternierendes-training-von-generator-und-kritiker" title="Link to this heading"></a></h2>
<p>Damit dieses Zero-Sum-Spiel überhaupt funktioniert, müssen Generator und Kritiker abwechselnd trainiert werden. Es ist nicht sinnvoll, beide Netzwerke gleichzeitig zu optimieren, da ihre Ziele direkt gegensätzlich sind. Stattdessen wird das Training in zwei Phasen aufgeteilt, die sich in jeder Iteration (oder jedem Batch) abwechseln:</p>
<ol class="arabic simple">
<li><p><strong>Trainingsschritt für den Kritiker (Discriminator):</strong></p>
<ul class="simple">
<li><p>Zunächst wird der Kritiker optimiert.</p></li>
<li><p>Er erhält echte Bilder aus dem Datensatz sowie vom Generator erzeugte (gefälschte) Bilder.</p></li>
<li><p>Ziel ist es, die Unterscheidbarkeit zwischen echten und generierten Bildern zu maximieren.</p></li>
<li><p>Die Loss-Funktion wird minimiert, wenn der Kritiker echten Bildern hohe und gefälschten Bildern niedrige Scores zuweist.</p></li>
</ul>
</li>
<li><p><strong>Trainingsschritt für den Generator:</strong></p>
<ul class="simple">
<li><p>Danach wird der Generator aktualisiert, während der Kritiker eingefroren bleibt.</p></li>
<li><p>Der Generator erzeugt neue Bilder aus Low-Resolution-Eingaben.</p></li>
<li><p>Ziel ist es nun, Bilder zu erzeugen, die der Kritiker fälschlicherweise als „echt“ klassifiziert.</p></li>
<li><p>Die Loss-Funktion wird minimiert, wenn der Generator es schafft, den Kritiker zu täuschen.</p></li>
</ul>
</li>
</ol>
<p>Dieses abwechselnde Training zwingt beide Netzwerke dazu, sich stetig weiterzuentwickeln: Der Kritiker wird darin besser, subtile Unterschiede zwischen real und generiert zu erkennen – und der Generator lernt, genau diese feinen Merkmale realistischer darzustellen.</p>
<p>Ein entscheidender Aspekt dieses Trainingsverfahrens ist das <strong>Gleichgewicht</strong>: Wird der Kritiker zu stark, kann der Generator kaum lernen, da er ständig „verlieren“ würde. Ist der Generator zu stark, lernt der Kritiker nichts mehr. Daher ist es in der Praxis üblich, mehrere Kritiker-Updates pro Generator-Update durchzuführen oder den Lernfortschritt beider Netzwerke sorgfältig zu überwachen.</p>
<p>Im nächsten Abschnitt zeigen wir, wie sich dieser Prozess konkret umsetzen lässt – sowohl algorithmisch als auch mit PyTorch-Code.</p>
</section>
<section id="trainingsschleife-fur-adversariales-upscaling">
<h2>Trainingsschleife für adversariales Upscaling<a class="headerlink" href="#trainingsschleife-fur-adversariales-upscaling" title="Link to this heading"></a></h2>
<p>Um das alternierende Training zwischen Generator und Kritiker effizient umzusetzen, strukturieren wir unsere Trainingsschleife in zwei Hauptblöcke pro Iteration: zuerst trainieren wir den Kritiker, dann den Generator. Dies ermöglicht es, die dynamische Balance zwischen beiden Netzwerken aufrechtzuerhalten und stabil zu lernen.</p>
<p>Der Ablauf in vereinfachter Form sieht wie folgt aus:</p>
<ol class="arabic simple">
<li><p>Hole einen Batch von korrespondierenden echten <strong>High-Resolution-Bildern und Low-Resolution-Bildern</strong> aus dem Trainingsdatensatz.</p></li>
<li><p><strong>Lasse den Generator hochskalierte Bilder erzeugen</strong>.</p></li>
<li><p><strong>Trainiere den Kritiker</strong> mit echten und generierten Bildern. Dabei soll die Punktzahl für echte Bilder maximiert und für generierte Bilder minimiert werden.</p></li>
<li><p><strong>Lipschitz-Bedingung</strong>: Wende Gewicht-Clipping oder Gradient Penalty an, um die Lipschitz-Bedingung zu gewährleisten.</p></li>
<li><p><strong>Trainiere den Generator</strong>, während der Kritiker eingefroren bleibt. Dabei soll die Punktzahl für die vom Generator erzeugten Bilder maximiert werden.</p></li>
</ol>
<section id="pseudocode-algorithmisch">
<h3>Pseudocode (algorithmisch):<a class="headerlink" href="#pseudocode-algorithmisch" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">real_lr_images</span><span class="p">,</span> <span class="n">real_hr_images</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>

        <span class="c1"># === Kritiker mehrfach updaten (n_critic Schritte) ===</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_critic</span><span class="p">):</span>
            <span class="n">freeze</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>
            <span class="n">unfreeze</span><span class="p">(</span><span class="n">critic</span><span class="p">)</span>

            <span class="n">fake_hr_images</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">real_lr_images</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

            <span class="c1"># Kritiker-Output: hohe Werte für echte Bilder, niedrige für generierte</span>
            <span class="n">critic_real</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">real_hr_images</span><span class="p">)</span>
            <span class="n">critic_fake</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">fake_hr_images</span><span class="p">)</span>

            <span class="n">d_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">critic_real</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">critic_fake</span><span class="p">)</span>

            <span class="n">d_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">optimizer_critic</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Gewicht-Clipping (Lipschitz-Bedingung. Alternative: Gradient Penalty)</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">critic</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="o">-</span><span class="n">clip_value</span><span class="p">,</span> <span class="n">clip_value</span><span class="p">)</span>

        <span class="c1"># === Generator-Update ===</span>
        <span class="n">freeze</span><span class="p">(</span><span class="n">critic</span><span class="p">)</span>
        <span class="n">unfreeze</span><span class="p">(</span><span class="n">generator</span><span class="p">)</span>

        <span class="n">generated_hr</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">real_lr_images</span><span class="p">)</span>
        <span class="n">critic_output</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">generated_hr</span><span class="p">)</span>

        <span class="c1"># Wasserstein-Loss (negativer Score, weil Generator maximieren will)</span>
        <span class="n">w_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">critic_output</span><span class="p">)</span>

        <span class="c1"># Optional: zusätzlicher Perceptual und TV-Loss</span>
        <span class="n">perceptual</span> <span class="o">=</span> <span class="n">compute_vgg_loss</span><span class="p">(</span><span class="n">generated_hr</span><span class="p">,</span> <span class="n">real_hr_images</span><span class="p">)</span>
        <span class="n">tv</span> <span class="o">=</span> <span class="n">total_variation_loss</span><span class="p">(</span><span class="n">generated_hr</span><span class="p">)</span>

        <span class="n">g_loss</span> <span class="o">=</span> <span class="n">λ_w</span> <span class="o">*</span> <span class="n">w_loss</span> <span class="o">+</span> <span class="n">λ_p</span> <span class="o">*</span> <span class="n">perceptual</span> <span class="o">+</span> <span class="n">λ_tv</span> <span class="o">*</span> <span class="n">tv</span>

        <span class="n">g_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer_generator</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="der-generator">
<h2>Der Generator<a class="headerlink" href="#der-generator" title="Link to this heading"></a></h2>
<p>Wir verwenden die gleiche Architektur wie im vorherigen Abschnitt, um hochskalierte Bilder zu erzeugen.
Der Generator nimmt Low-Resolution-Bilder als Eingabe und gibt hochskalierte Bilder zurück.
Wir verwenden zunächst eine Kaskade von ResNet-Blöcken, gefolgt von einem Upsampling-Schritt mit PixelShuffle, um die Auflösung zu erhöhen.
Dabei verwenden wir stets 7x7-Kernel, um die Details zu erhalten und die Bilder realistisch zu gestalten. Wir beginnen mit 16 Kanälen und verdoppeln
die Anzahl der Kanäle in jedem Block, um die Komplexität zu erhöhen. Beim Upsampling verwenden wir PixelShuffle mit einem Skalierugsfaktor von 4,
um die Auflösung zu erhöhen und die Anzahl der Kanäle zu reduzieren. Die verbleibenden 16 Kanäle in voller Auflösung werden dann durch eine weitere
klassischen Faltung mit einer 7x7 Maske auf 3 Kanäle reduziert, um das finale hochskalierte Bild zu erzeugen. Die letzte Faltung verwendet keine
Aktivierungsfunktion sondern wird wieder wie vorher zu dem klassisch hoch-skalierten Bild (Bilinear Upsampling) addiert.</p>
<a class="reference internal image-reference" href="../_images/generator.png"><img alt="../_images/generator.png" class="align-center" src="../_images/generator.png" style="width: 600px;" />
</a>
<p>Der ResNet-Block ist bereits implementiert und kann verwendet werden.</p>
<dl class="py method">
<dt class="sig sig-object py" id="adversarialloss.misc.ResNetBlock.__init__">
<span class="sig-prename descclassname"><span class="pre">ResNetBlock.</span></span><span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/adversarialloss/misc.html#ResNetBlock.__init__"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#adversarialloss.misc.ResNetBlock.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialisiert einen ResNet-Block mit zwei Convolutional-Schichten, Batch-Normalisierung und ReLU-Aktivierung.</p>
<section id="parameters">
<h3>Parameters:<a class="headerlink" href="#parameters" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>in_channels (int):</dt><dd><p>Anzahl der Eingabekanäle.</p>
</dd>
<dt>out_channels (int):</dt><dd><p>Anzahl der Ausgabekanäle.</p>
</dd>
<dt>kernel_size (int, optional):</dt><dd><p>Größe des Convolutional-Kernels. Standard ist 9.</p>
</dd>
<dt>padding (int, optional):</dt><dd><p>Padding für die Convolutional-Schichten. Standard ist None. In dem Fall wird das Padding automatisch berechnet, so dass die Ausgabe die gleiche Größe wie die Eingabe hat.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
<section id="aufgabe-1-generator-architektur-implementieren">
<h2><strong>Aufgabe 1</strong>: Generator-Architektur implementieren<a class="headerlink" href="#aufgabe-1-generator-architektur-implementieren" title="Link to this heading"></a></h2>
<p>Implementieren Sie nun die Generator-Klasse, welche die ResNet-Blöcke verwendet und die oben beschriebene Architektur umsetzt.</p>
<dl class="py class">
<dt class="sig sig-object py" id="adversarialloss.main.Generator">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">adversarialloss.main.</span></span><span class="sig-name descname"><span class="pre">Generator</span></span><a class="reference internal" href="../_modules/adversarialloss/main.html#Generator"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#adversarialloss.main.Generator" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="adversarialloss.main.Generator.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/adversarialloss/main.html#Generator.__init__"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#adversarialloss.main.Generator.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize the Upscale4x model.</p>
<p>This model performs 4x upscaling using a series of ResNet blocks and an upsampling layer.</p>
<p><strong>TODO</strong>:</p>
<ul class="simple">
<li><p>Call the <cite>__init__</cite> method of the base class <cite>nn.Module</cite>.</p></li>
<li><p>Define an upsampling layer using <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Upsample.html">nn.Upsample(scale_factor=4, mode=“bilinear“, align_corners=True)</a>.</p></li>
<li><p>Define a sequential model consisting of:</p></li>
<li><p>Five <cite>ResNetBlock</cite> layers with 3-&gt;16, 16-&gt;32, 32-&gt;64, 64-&gt;128 and 128-&gt;256 channels as well as kernel sizes 7.</p></li>
<li><p>A PixelShuffle layer with an upscale factor of 4.</p></li>
<li><p>A final convolutional layer with 16 input channels, 3 output channels and kernel size 5 with padding 2.</p></li>
</ul>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="adversarialloss.main.Generator.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/adversarialloss/main.html#Generator.forward"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#adversarialloss.main.Generator.forward" title="Link to this definition"></a></dt>
<dd><p>Perform the forward pass of the Upscale2x model.</p>
<section id="id1">
<h3>Parameters:<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<blockquote>
<div><dl class="simple">
<dt>x (torch.Tensor):</dt><dd><p>The input tensor to be upscaled.</p>
</dd>
</dl>
</div></blockquote>
</section>
<section id="returns">
<h3>Returns:<a class="headerlink" href="#returns" title="Link to this heading"></a></h3>
<blockquote>
<div><dl class="simple">
<dt>torch.Tensor:</dt><dd><p>The upscaled output tensor.</p>
</dd>
</dl>
</div></blockquote>
<p><strong>TODO</strong>:</p>
<ul class="simple">
<li><p>Pass the input tensor through the model.</p></li>
<li><p>Also, apply the upsampling layer to the input tensor <cite>x</cite>.</p></li>
<li><p>Add the upsampled tensor to the output of the model.</p></li>
</ul>
</section>
</dd></dl>

</dd></dl>

<div class="toggle admonition">
<p class="admonition-title">Lösung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">upBilinear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Upsample</span><span class="p">(</span>
        <span class="n">scale_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bilinear&quot;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">ResNetBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span>
        <span class="n">ResNetBlock</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span>
        <span class="n">ResNetBlock</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span>
        <span class="n">ResNetBlock</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span>
        <span class="n">ResNetBlock</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">PixelShuffle</span><span class="p">(</span><span class="n">upscale_factor</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>  <span class="c1"># First upsample</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>  <span class="c1"># Final conv to reduce channels</span>
    <span class="p">)</span>

  <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">upBilinear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../perceptualloss/index.html" class="btn btn-neutral float-left" title="Perceptual Loss" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Zurück</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Prof. Dr. Dennis Müller.</p>
  </div>

  Erstellt mit <a href="https://www.sphinx-doc.org/">Sphinx</a> mit einem
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    bereitgestellt von <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>