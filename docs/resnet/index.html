

<!DOCTYPE html>
<html class="writer-html5" lang="de" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ResNet &mdash; Advances in AI Praktikum  Dokumentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />

  
    <link rel="canonical" href="https://dmu1981.github.io/MPTPraktikum/resnet/index.html" />
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=245627df"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script src="../_static/translations.js?v=79cc9f76"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Stichwortverzeichnis" href="../genindex.html" />
    <link rel="search" title="Suche" href="../search.html" />
    <link rel="prev" title="TensorBoard" href="../tensorboard/index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Advances in AI Praktikum
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Dokumentation durchsuchen" aria-label="Dokumentation durchsuchen" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Aufgaben:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../pytorch/index.html">PyTorch - Grundlagen</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/cnn.html">Convolutional Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../checkpoints/index.html">Checkpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorboard/index.html">Tensor Board</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">ResNet</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#aufgabe-1-resnet-block-implementieren"><strong>Aufgabe 1</strong>: ResNet-Block implementieren</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#resnet.ResidualBlock"><code class="docutils literal notranslate"><span class="pre">ResidualBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resnet.ResidualBlock.__init__"><code class="docutils literal notranslate"><span class="pre">ResidualBlock.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#resnet.ResidualBlock.forward"><code class="docutils literal notranslate"><span class="pre">ResidualBlock.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#aufgabe-2-ein-einfaches-resnet-modell-implementieren"><strong>Aufgabe 2</strong>: Ein einfaches ResNet-Modell implementieren</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#resnet.ResNet.make_layer"><code class="docutils literal notranslate"><span class="pre">ResNet.make_layer()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Advances in AI Praktikum</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ResNet</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/resnet/index.rst.txt" rel="nofollow"> Quelltext anzeigen</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="resnet">
<h1>ResNet<a class="headerlink" href="#resnet" title="Link to this heading"></a></h1>
<p>In diesem Abschnitt finden Sie die Aufgaben und Beispiele zur Implementierung von ResNet-Architekturen in PyTorch.
Die ResNet-Architektur ist bekannt für ihre Fähigkeit, tiefe neuronale Netzwerke zu trainieren, indem sie
Residualverbindungen verwendet, die den Gradientenfluss verbessern und das Problem der verschwindenden
Gradienten verringern.</p>
<p>ResNet wurde Ursprünglich von Kaiming He et al. in ihrem Paper „<a class="reference external" href="https://arxiv.org/pdf/1512.03385">Deep Residual Learning for Image Recognition</a>“
vorgestellt. Es hat sich als äußerst effektiv erwiesen und wurde in vielen Anwendungen der Computer Vision
eingesetzt.</p>
<section id="aufgabe-1-resnet-block-implementieren">
<h2><strong>Aufgabe 1</strong>: ResNet-Block implementieren<a class="headerlink" href="#aufgabe-1-resnet-block-implementieren" title="Link to this heading"></a></h2>
<p>Wir implementieren zunächst den grundlegenden ResNet-Block, der die Residualverbindung enthält.
Aus diesem Block werden dann die tieferen ResNet-Architekturen aufgebaut.</p>
<a class="reference internal image-reference" href="../_images/resnet-block.svg"><img alt="../_images/resnet-block.svg" class="align-center" src="../_images/resnet-block.svg" style="width: 600px;" />
</a>
<p>Der ResNet-Block besteht aus zwei aufeinanderfolgenden Convolutional-Layern, gefolgt von Batch Normalization und ReLU-Aktivierung.
Die Residualverbindung addiert den Eingang des Blocks zu der Ausgabe der zweiten Convolutional-Schicht.</p>
<p>Unser Netzwerk wird zwei Varianten des ResNet-Blocks enthalten. Die linke Architektur zeigen den
ResNet-Block ohne Downsampling (Stride). Die Anzahl der Kanäle bleibt gleich.</p>
<p>Die rechte Architektur zeigt den ResNet-Block mit Downsampling, bei dem gleichzeitig die Anzahl der Kanäle verdoppelt wird.
Damit die Residualverbindung funktioniert, muss der Eingang des Blocks auf die gleiche Größe wie die Ausgabe transformiert werden.
Dazu verwenden wir eine 1x1 Convolutional-Schicht, die die Anzahl der Kanäle anpasst und den selben Stride wie die Convolutional-Schicht verwendet.</p>
<p>Alle Faltungen sind mit einer Kernelgröße von 3x3 und Padding von 1 konfiguriert, um die räumliche Größe der Eingabe beizubehalten.
Sie verwenden ausserdem keinen Bias.</p>
<p>Implementieren Sie die Klasse <cite>ResidualBlock</cite> in der Datei <cite>resnet/resnet.py</cite>, die den ResNet-Block mit und ohne Downsampling enthält.</p>
<dl class="py class">
<dt class="sig sig-object py" id="resnet.ResidualBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">resnet.</span></span><span class="sig-name descname"><span class="pre">ResidualBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/resnet.html#ResidualBlock"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#resnet.ResidualBlock" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="resnet.ResidualBlock.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/resnet.html#ResidualBlock.__init__"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#resnet.ResidualBlock.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialisiert einen Residual Block.</p>
<section id="parameters">
<h3>Parameters:<a class="headerlink" href="#parameters" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>in_channels (int):</dt><dd><p>Anzahl der Eingabekanäle.</p>
</dd>
<dt>out_channels (int):</dt><dd><p>Anzahl der Ausgabekanäle.</p>
</dd>
<dt>stride (int):</dt><dd><p>Schrittweite für die Faltung. Standard ist 1.</p>
</dd>
</dl>
<p><strong>TODO</strong>:</p>
<ul class="simple">
<li><p>Rufen Sie die <cite>__init__</cite> Methode der Basisklasse <cite>nn.Module</cite> auf.</p></li>
<li><p>Initialisieren Sie dann die Schichten des Residual Blocks.</p></li>
<li><p>Verwenden Sie <cite>nn.Conv2d</cite> für die Faltungsschichten. Setzen Sie <cite>kernel_size=3</cite>, <cite>padding=1</cite> und <cite>bias=False</cite>.</p></li>
<li><p>Die erste Faltungsschicht sollte <cite>in_channels</cite> zu <cite>out_channels</cite> transformieren, die zweite Faltungsschicht sollte <cite>out_channels</cite> zu <cite>out_channels</cite> transformieren.</p></li>
<li><p>Die ersten Faltungsschicht sollte <cite>stride</cite> als Schrittweite verwenden.</p></li>
<li><p>Fügen Sie <cite>nn.BatchNorm2d</cite> nach jeder Faltungsschicht hinzu. Achten Sie darauf, dass die Batch-Normalisierung die gleiche Anzahl an Ausgabekanälen wie die Faltungsschicht hat.</p></li>
<li><p>Verwenden Sie <cite>nn.ReLU</cite> als Aktivierungsfunktion.</p></li>
<li><p>Implementieren Sie die Shortcut-Verbindung. Wenn <cite>stride</cite> nicht 1 ist oder <cite>in_channels</cite> nicht gleich <cite>out_channels</cite>, verwenden Sie eine 1x1 Faltung, um die Dimensionen anzupassen. Andernfalls verwenden Sie <cite>nn.Identity()</cite>.</p></li>
</ul>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="resnet.ResidualBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/resnet.html#ResidualBlock.forward"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#resnet.ResidualBlock.forward" title="Link to this definition"></a></dt>
<dd><p>Führt den Vorwärtsdurchlauf des Residual Blocks aus.</p>
<section id="id1">
<h3>Parameters:<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>x (torch.Tensor):</dt><dd><p>Eingabetensor.</p>
</dd>
</dl>
<p><strong>TODO</strong>:
Implementieren Sie den Vorwärtsdurchlauf des Residual Blocks.
Orientieren Sie sich an der in der Aufgabenstellung gegebenen Beschreibung sowie der Grafik.</p>
</section>
</dd></dl>

</dd></dl>

<div class="toggle admonition">
<p class="admonition-title">Musterlösung für den Konstruktur __init__ anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
       <span class="nb">super</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">),</span>
       <span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
           <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
       <span class="p">)</span>
       <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

       <span class="c1"># Shortcut connection</span>
       <span class="k">if</span> <span class="n">stride</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">out_channels</span><span class="p">:</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
       <span class="k">else</span><span class="p">:</span>
           <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="toggle admonition">
<p class="admonition-title">Musterlösung für den Forwärtspass anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">residual</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">+</span> <span class="n">residual</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</div>
</section>
<section id="aufgabe-2-ein-einfaches-resnet-modell-implementieren">
<h2><strong>Aufgabe 2</strong>: Ein einfaches ResNet-Modell implementieren<a class="headerlink" href="#aufgabe-2-ein-einfaches-resnet-modell-implementieren" title="Link to this heading"></a></h2>
<p>Nachdem wir den ResNet-Block implementiert haben, können wir ein einfaches ResNet-Modell erstellen.
Das Modell besteht aus mehreren ResNet-Blöcken, die in verschiedenen Konfigurationen angeordnet sind.</p>
<p>Die Architektur, die wir in diesem Abschnitt implementieren, ist eine an den CIFAR-10-Datensatz angepasste Version von ResNet.
Wir verwenden insgesamt 25 Faltungsschichten, die in 4 Blöcken organisiert sind.</p>
<a class="reference internal image-reference" href="../_images/architecture.png"><img alt="../_images/architecture.png" class="align-center" src="../_images/architecture.png" style="width: 600px;" />
</a>
<p>Grundsätzlichen werden alle Convolutional-Schichten der Architektur von einer Batch Normalization-Schicht gefolgt,
die die Normalisierung der Aktivierungen ermöglicht und die Trainingsstabilität verbessert. Als Nicht-Linearität verwenden wir die ReLU-Aktivierungsfunktion
nach jeder Batch-Normalization-Schicht. Alle Convolutional-Schichten verwenden keinen Bias da dieser durch die Batch Normalization-Schicht ersetzt wird.</p>
<p>In der ersten Schicht (grün) verwenden wir die einzige 7x7 Convolutional-Schicht des Netzwerkes.
Die 3 Eingangskanäle werden auf 32 Kanäle erweitert. Gleichzeit verringert ein Stride von 2 die räumliche Dimension der Eingabe um die Hälfte.</p>
<p>In der folgenden Schichten verwenden wir 6 aufeinanderfolgende ResNet-Blöcke (pink), die jeweils 32 Kanäle haben.
Die Shortcut-Verbindung jedes ResNet-Blocks ermöglicht einen stabilen Gradientenfluss durch das Netzwerk.
Die Anzahl der Kanäle bleibt in diesen Blöcken konstant, da sie keinen Downsampling-Schritt enthalten.</p>
<p>Die nächsten 6 ResNet-Blöcke (blau) verdoppeln die Anzahl der Kanäle auf 64 und verwenden einen Downsampling-Schritt mit einem Stride von 2.
Das Downsampling sowie die Dimensionserhöhung erfolgt im ersten der sechs Blöcke. Die restlichen fünf Blöcke verwenden die gleiche Anzahl von Kanälen und
behalten die räumliche Dimension bei.</p>
<p>Die letzte Stufe besteht aus insgesamt 12 ResNet-Blöcken (orange), wobei wieder die Anzahl der Kanäle im ersten Block auf 128 verdoppelt wird während eine
Stride von 2 im ersten Block die räumliche Auflösung wieder halbiert.</p>
<p>Das Netzwerk endet mit einer globalen Durchschnittspooling-Schicht, die die räumliche Dimension auf 1x1 reduziert.
Anschließend folgt eine voll verbundene Schicht, die die Anzahl der Kanäle auf 10 reduziert, um die 10 Klassen des
CIFAR-10-Datensatzes zu klassifizieren.</p>
<p><strong>Implementieren</strong> Sie nun zunächst die Methode <cite>Resnet._make_layer</cite> in der Datei <cite>resnet/resnet.py</cite>, die eine Sequenz von ResNet-Blöcken erstellt.</p>
<dl class="py method">
<dt class="sig sig-object py" id="resnet.ResNet.make_layer">
<span class="sig-prename descclassname"><span class="pre">ResNet.</span></span><span class="sig-name descname"><span class="pre">make_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_blocks</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/resnet.html#ResNet.make_layer"><span class="viewcode-link"><span class="pre">[Quellcode]</span></span></a><a class="headerlink" href="#resnet.ResNet.make_layer" title="Link to this definition"></a></dt>
<dd><p>Erstellt eine Sequenz von Residual Blocks.</p>
<section id="id2">
<h3>Parameters:<a class="headerlink" href="#id2" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>in_channels (int):</dt><dd><p>Anzahl der Eingabekanäle.</p>
</dd>
<dt>out_channels (int):</dt><dd><p>Anzahl der Ausgabekanäle.</p>
</dd>
<dt>num_blocks (int):</dt><dd><p>Anzahl der Residual Blocks in dieser Schicht.</p>
</dd>
<dt>stride (int):</dt><dd><p>Schrittweite für die erste Faltungsschicht des ersten Blocks.</p>
</dd>
</dl>
</section>
<section id="returns">
<h3>Returns:<a class="headerlink" href="#returns" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>nn.Sequential:</dt><dd><p>Eine Sequenz von Residual Blocks.</p>
</dd>
</dl>
<p><strong>TODO</strong>:</p>
<ul class="simple">
<li><p>Erstellen Sie eine Liste von Schichten, die die Residual Blocks enthalten.</p></li>
<li><p>Die erste Schicht sollte einen Residual Block mit <cite>in_channels</cite>, <cite>out_channels</cite> und <cite>stride</cite> sein.</p></li>
<li><p>Die folgenden Schichten sollten Residual Blocks mit gleichbleibender Kanalanzahl sein. Verwenden Sie <cite>out_channels</cite> sowohl für die Eingabe- als auch für die Ausgabekanäle.</p></li>
<li><p>Verwenden Sie <cite>nn.Sequential</cite> um die Schichten zu kombinieren und zurückzugeben.</p></li>
</ul>
<p><strong>Hinweis</strong>:</p>
<ul class="simple">
<li><p>Die erste Schicht sollte die Schrittweite <cite>stride</cite> verwenden, während die anderen Schichten eine Schrittweite von 1 haben.</p></li>
<li><p>Sie können die gewünschten Layer mit <cite>nn.Sequential</cite> kombinieren.</p></li>
<li><p>Dazu können Sie die Blöcke zunächst in einer Liste (z.B. <cite>layers</cite>) sammeln und dann <cite>nn.Sequential(*layers)</cite> verwenden, um sie zu kombinieren.</p></li>
</ul>
</section>
</dd></dl>

<div class="toggle admonition">
<p class="admonition-title">Musterlösung anzeigen</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">make_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">block</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
   <span class="n">strides</span> <span class="o">=</span> <span class="p">[</span><span class="n">stride</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
   <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
   <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">strides</span><span class="p">:</span>
       <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualBlock</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">s</span><span class="p">))</span>
       <span class="n">in_channels</span> <span class="o">=</span> <span class="n">out_channels</span>

   <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../tensorboard/index.html" class="btn btn-neutral float-left" title="TensorBoard" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Zurück</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Prof. Dr. Dennis Müller.</p>
  </div>

  Erstellt mit <a href="https://www.sphinx-doc.org/">Sphinx</a> mit einem
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    bereitgestellt von <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>