Checkpoints
===========

Um Zwischenstände während des Trainings zu speichern und 
später wiederherzustellen, können Checkpoints verwendet werden. 
Diese ermöglichen es, den Trainingsprozess zu unterbrechen und später 
fortzusetzen, ohne von vorne beginnen zu müssen oder um den besten Zustand des 
Modells zu sichern. PyTorch bietet eine einfache Möglichkeit, Checkpoints zu erstellen.

